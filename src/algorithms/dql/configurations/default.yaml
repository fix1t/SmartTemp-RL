network:
  seed: 42
  hidden_layers:
    - 64
    - 64
  activation: "ReLU"
  output_activation: "Softmax"

hyperparameters:
  learning_rate: 0.001
  discount_factor: 0.99
  epsilon: 0.1
  batch_size: 32
  epsilon_starting_value: 1.0
  epsilon_ending_value: 0.01
  epsilon_decay_value: 0.995
  learning_rate: 0.0005
  batch_size: 100
  discount_factor: 0.99
  replay_buffer_size: 100000
  interpolation_parameter: 0.001